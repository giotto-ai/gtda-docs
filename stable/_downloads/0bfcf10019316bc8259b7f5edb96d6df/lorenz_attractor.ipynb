{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nCase study: Lorenz attractor\n============================\n\nLicense: AGPLv3\n'''''''''''''''\n\nThis notebook contains a full TDA pipeline to analyse the transitions of\nthe Lorenz system to a chaotic regime from the stable one and viceversa.\n\nThe first step consists in importing relevant *gtda* components and\nother useful libraries or modules.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Import the gtda module\nfrom gtda.time_series import Resampler, TakensEmbedding, SlidingWindow, PermutationEntropy\nfrom gtda.homology import VietorisRipsPersistence\nfrom gtda.diagrams import Scaler, Filtering, PersistenceEntropy, BettiCurve, PairwiseDistance\nfrom gtda.graphs import KNeighborsGraph, GraphGeodesicDistance\n\nfrom gtda.pipeline import Pipeline\n\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\nimport matplotlib.pyplot as plt\n\nimport plotly.graph_objs as gobj\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\n\n# Import data from openml\nimport openml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plotting functions\n==================\n\nThe *plotting.py* file is required to use the following plotting\nfunctions. It can be found in the *examples* folder on out github.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Plotting functions\nfrom plotting import plot_diagram, plot_landscapes\nfrom plotting import plot_betti_surfaces, plot_betti_curves\nfrom plotting import plot_point_cloud"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Setting up the Lorenz attractor simulation\n==========================================\n\nIn the next block we set up all the parameters of the Lorenz system and\nwe define also the instants at which the regime (stable VS chaotic)\nchanges.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Plotting the trajectories of the Lorenz system\nfrom openml.datasets.functions import get_dataset\n\npoint_cloud = get_dataset(42182).get_data(dataset_format='array')[0]\nplot_point_cloud(point_cloud)\n\n# Selecting the z-axis and the label rho\nX = point_cloud[:,2].reshape(-1, 1)\ny = point_cloud[:,3]\n\n# Plotting the Lorenz system\nfig = plt.figure(figsize=(16,6))\nplt.plot(X)\nplt.plot(y)\nplt.title('Trajectory of the Lorenz solution, projected along the z-axis')\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Resampling the time series\n==========================\n\nIt is important to find the correct time scale at which key signals take\nplace. Here we propose one possible resampling period: *10h*. Recall\nthat the unit time is *1h*. The resampler method is used to perform the\nresampling.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "period = 10\nperiodicSampler = Resampler(period=period)\n\nperiodicSampler.fit(X)\nX_sampled, y_sampled = periodicSampler.transform_resample(X, y)\n\nfig = plt.figure(figsize=(16,6))\nplt.plot(X_sampled)\nplt.plot(y_sampled)\nplt.title('Trajectory of the Lorenz solution, projected along the z-axis and resampled every 10h')\nplt.show()\n\n# Steps of the Pipeline\nsteps = [\n    ('sampling', Resampler(period=period)),\n    ('regressor', RandomForestRegressor(n_estimators=100))\n]\n\n# Sklearn Pipeline\npipeline = Pipeline(steps)\n\npipeline.fit(X, y);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Takens Embedding\n================\n\nIn order to obtain meaningful topological features from a time series,\nwe use a delayed-time embedding technique, invented by F. Takens in the\nlate sixties. The idea is simple: given a time series X(t), one can\nextract a sequence of vectors of the form X_i := [(X(t_i)), X(t_i + 2\ntau), \u2026, X(t_i + M tau)]. The difference between t_i and t_i-1 is called\n*stride*; the numbers M and tau are optimized authomatically in this\nexample (they can be set by the user if needed).\n\nThe *outer window* allows us to apply Takens embedding locally on a\ncertain interval rather than over the whole time series. The result of\nthis procedure is therefore a time series of point clouds with possibly\ninteresting topologies.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "embedding_dimension = 10\nembedding_time_delay = 3\nembedder = TakensEmbedding(parameters_type='search', dimension=embedding_dimension,\n                           time_delay=embedding_time_delay, n_jobs=-1)\n\nembedder.fit(X)\nembedder_time_delay = embedder.time_delay_\nembedder_dimension = embedder.dimension_\n\nprint('Optimal embedding time delay based on mutual information: ', embedder_time_delay)\nprint('Optimal embedding dimension based on false nearest neighbors: ', embedder_dimension)\n\nX_embedded, y_embedded = embedder.transform_resample(X_sampled, y_sampled)\n\nwindow_width = 40\nwindow_stride = 5\nsliding_window = SlidingWindow(width=window_width, stride=window_stride)\nsliding_window.fit(X_embedded, y_embedded)\n\nX_windows, y_windows = sliding_window.transform_resample(X_embedded, y_embedded)\n\n# Plotting Takens embedding of the third outer window\nwindow_number = 3\nwindow = X_windows[window_number][:,:3]\nplot_point_cloud(window)\n\n# Plotting the time series associated to the above point cloud. \n# Notice the two periodicities corresponding to the loops in the embedding\nfig = plt.figure(figsize=(16,6))\nplt.plot(X_sampled[(window_number*window_stride)*embedder_time_delay*embedder_dimension:\n                   (window_number*window_stride + window_width)*embedder_time_delay*embedder_dimension])\nplt.title('The Lorenz solution over one outer window ')\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Persistence diagram\n===================\n\nThe topological information of the correlation metric is synthesised in\nthe persistent diagram. The horizonral axis corresponds to the moment in\nwhich an homological generator is born, while the vertical axis\ncorresponds to the moments in which an homological generator dies. The\ngenerators of the homology groups (at given rank) are colored\ndifferently.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "homology_dimensions = [0, 1, 2]\npersistenceDiagram = VietorisRipsPersistence(metric='euclidean', max_edge_length=100,\n                                             homology_dimensions=homology_dimensions, n_jobs=1)\npersistenceDiagram.fit(X_windows)\nX_diagrams = persistenceDiagram.transform(X_windows, y_windows)\n\n# Plot the persistence diagram\nplot_diagram(X_diagrams[window_number])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Scikit-learn *Pipeline*\n=======================\n\nOne of the advantages of giotto-tda is the compatibility with\nscikit-learn. It is possible to set up a full pipeline such as the one\nabove in a few lines. We will show how in the next section.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Steps of the Pipeline\nsteps = [\n    ('sampling', Resampler(period=period)),\n    ('embedding', TakensEmbedding(parameters_type='search', dimension=embedding_dimension,\n                                  time_delay=embedding_time_delay, n_jobs=-1)),\n    ('window', SlidingWindow(width=window_width, stride=window_stride)),\n    ('diagrams', VietorisRipsPersistence(metric='euclidean', max_edge_length=100, \n                                            homology_dimensions=homology_dimensions, n_jobs=-1))\n]\n\n# Sklearn Pipeline\npipeline = Pipeline(steps)\n\n# Running the pipeline\npipeline.fit(X)\nX_diagrams = pipeline.transform(X)\n\n# Plotting the final persistent diagram of one outer window\nplot_diagram(X_diagrams[window_number])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Rescaling the diagram\n=====================\n\nRescaling the diagram means normalizing the points such that their\ndistance from the *empty diagram* is equal to one. Once the diagram is\nrescaled, we can filter noise by removing all the points that are close\nto the diagonal.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "diagram_scaler = Scaler()\ndiagram_scaler.fit(X_diagrams)\nX_scaled = diagram_scaler.transform(X_diagrams)\n\nplot_diagram(X_scaled[window_number])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Filtering diagrams\n==================\n\nFiltering noise from a diagram corresponds to eliminating all the\nhomology generators whose lifespan is too short to be significant. It\nmeans equivalently that we are removing the points closer to the\ndiagonal than a centrain threshold.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Preparing the filtering transformer\ndiagram_filter = Filtering(epsilon=0.1, homology_dimensions=[1, 2])\ndiagram_filter.fit(X_scaled)\nX_filtered = diagram_filter.transform(X_scaled)\n\nplot_diagram(X_filtered[window_number])\n\n# Wrapping up all the steps inside a scikit-learn pipeline\nsteps = [\n    ('sampling', Resampler(period=period)),\n    ('embedding', TakensEmbedding(parameters_type='search', dimension=embedding_dimension, \n                                    time_delay=embedding_time_delay, n_jobs=1)),\n    ('window', SlidingWindow(width=window_width, stride=window_stride)),\n    ('diagrams', VietorisRipsPersistence(metric='euclidean', max_edge_length=100, \n                                            homology_dimensions=homology_dimensions, n_jobs=1)),\n    ('diagrams_scaler', Scaler()),\n    ('diagrams_filter', Filtering(epsilon=0.1))\n]\n\npipeline_filter = Pipeline(steps)\n\npipeline_filter.fit(X)\nX_filtered_noisy = pipeline_filter.transform(X)\n\nplot_diagram(X_filtered_noisy[window_number])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Persistence entropy\n===================\n\nIn this section we show how to compute the entropy of persistence\ndiagrams.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Perparing the filtering transformer\npersistent_entropy = PersistenceEntropy()\npersistent_entropy.fit(X_scaled)\nX_persistent_entropy = persistent_entropy.transform(X_scaled)\n\nfig = plt.figure(figsize=(16,6))\nplt.plot(X_persistent_entropy)\nplt.title('Trajectory of the Lorenz solution, projected along the z-axis')\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Betti Curves\n============\n\nIn this section we show how to compute the Betti curves of a persistent\ndiagram. We also show the plot of the Betti surface, i.e.\u00a0the time-stack\nof the bettu curves.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "betti_curves = BettiCurve()\nbetti_curves.fit(X_scaled)\nX_betti_curves = betti_curves.transform(X_scaled)\n\nplot_betti_curves(X_betti_curves[window_number])\n\nplot_betti_surfaces(X_betti_curves)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Distances among diagrams\n========================\n\nIn this section we show how to compute distances among persistent\ndiagrams. There are many notions of distances: here we use the\n*bottleneck distance* as an example.\n\nWe stress that the *i-th* row of this matrix is the distance of the\n*i-th* diagram from all the others.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# We change metric: we compute the landscape distance among the diagrams \ndiagram_distance = PairwiseDistance(metric='landscape', metric_params={'p': 2, 'n_layers':1, 'n_bins':1000}, \n                                       order=None, n_jobs=2)\ndiagram_distance.fit(X_diagrams)\nX_distance_L = diagram_distance.transform(X_diagrams)\n\n# Plot of the landascape L2 distance between persistent diagrams \nfigure = plt.figure(figsize=(10,10))\nplt.imshow(X_distance_L[:,:, 0])\nplt.colorbar()\nplt.title('Landscape L2 distance matrix for H0')\nplt.show()\n\n# We change metric: we compute the wasserestein distance among the diagrams \ndiagram_distance = PairwiseDistance(metric='wasserstein', metric_params={'p': 2, 'delta':0.2},\n                                    order=2, n_jobs=1)\ndiagram_distance.fit(X_diagrams)\nX_distance_W = diagram_distance.transform(X_diagrams)\n\n# Plot of the landascape L2 distance between persistent diagrams \nfigure = plt.figure(figsize=(10,10))\nplt.imshow(X_distance_W)\nplt.colorbar()\nplt.title('2-norm of 2-wassertein distance matrices')\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "New distance in the embedding space: permutations and graphs\n============================================================\n\nWe propose here a new way to compute distances between points in the\nembedding space. Instead of considering the Euclidean distance in the\nTakens space, we propose to build a k-nearest neightbors graph and then\nuse the geodesic distance on such graph.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "kNN_graph = KNeighborsGraph(n_neighbors=3, n_jobs=1)\nkNN_graph.fit(X_embedded)\nX_kNN = kNN_graph.transform(X_embedded[0].reshape((1, X_embedded.shape[1], -1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Geodesic distance on graphs\n===========================\n\nGiven the graph embedding, the natural notion of distance between\nvertices corresponds to the lengths of the shortest path connecting two\nvertices. This is also known as *graph geodesic distance*.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "geodesic_distance = GraphGeodesicDistance(n_jobs=1)\ngeodesic_distance.fit(X_kNN)\nX_GeoNN = geodesic_distance.transform(X_kNN)\n\n# Plotting the geodesic distance\nfigure = plt.figure(figsize=(10,10))\nplt.imshow(X_GeoNN[0])\nplt.colorbar()\nplt.title('Plot of the geodesic distance of the first outer window kNN graph')\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Synthetic features extracted from topology\n==========================================\n\nOne can use persistent diagrams as he pleases. Here we show one way of\nextracting summary information from the time-series of diagrams: the\n*permutation entropy*. The entropy is computed by estimating a\nprobability based on teh counting how many permutation patterns appear\nalong the time series within the outer window.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Computing permutation entropy\npermutation_entropy = PermutationEntropy(n_jobs=1)\npermutation_entropy.fit(X_windows)\nX_entropy = permutation_entropy.transform(X_windows)\n\n# Plot the sampled Lorenz solution, projected along the z-axis\nfig = plt.figure(figsize=(16,6))\nplt.plot(X_sampled)\nplt.title('Resampled solution of the Lorenz attractor projected along the z-axis')\nplt.show()\n\n# Plot of the permutation entropy time-series\nfig = plt.figure(figsize=(16,6))\nplt.plot(X_entropy)\nplt.title('Permutation entropy')\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}